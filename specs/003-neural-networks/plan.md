# Implementation Plan: Neural Network Perception System

**Branch**: `003-neural-networks` | **Date**: 2025-11-21 | **Spec**: [spec.md](./spec.md)
**Input**: Feature specification from `/specs/003-neural-networks/spec.md`

**Note**: This template is filled in by the `/speckit.plan` command. See `.specify/templates/commands/plan.md` for the execution workflow.

## Summary

Implement neural network perception system for YouBot autonomous robot using LIDAR obstacle detection and camera-based cube color classification. System processes LIDAR point clouds (667 points, 270¬∞ FOV) through neural network for obstacle mapping and camera images (512√ó512 RGB) through CNN for cube detection (green/blue/red). Technical approach: MLP or PointNet for LIDAR, custom CNN or pre-trained model for camera, PyTorch for training/inference, real-time integration in Webots controller. Target performance: >90% obstacle detection, >95% color classification, <100ms inference time, >10 FPS camera processing.

## Technical Context

**Language/Version**: Python 3.8+ (Webots R2023b controller requirement)
**Primary Dependencies**: PyTorch 2.0+, NumPy, SciPy, OpenCV, Matplotlib, scikit-learn
**Storage**: File-based (models/*.pth for trained models, data/ for datasets, logs/ for experiment tracking)
**Testing**: pytest (unit tests), custom validation scripts (accuracy metrics), Webots integration tests
**Target Platform**: Webots R2023b simulator on macOS/Linux, CPU inference (GPU optional for training)
**Project Type**: Single (robotics perception system integrated into existing youbot controller)
**Performance Goals**:
  - LIDAR: <100ms inference time, >90% obstacle detection accuracy
  - Camera: >10 FPS processing, >95% color classification accuracy
  - Combined: <150ms end-to-end latency, 5-min continuous operation without crashes
**Constraints**:
  - Real-time operation in 32ms Webots timestep
  - CPU-only inference in final demo (GPU for training acceptable)
  - Model size <50MB each for practical loading
  - No GPS allowed in final demonstration (project rule)
**Scale/Scope**:
  - 2 neural networks (LIDAR processor + camera detector)
  - Training datasets: >1000 LIDAR scans, >500 camera images
  - 7 key entities/classes (LIDARProcessor, CubeDetector, PerceptionSystem, etc.)
  - Integration with existing Phase 1 controllers (base, arm, gripper)

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

### ‚úÖ Core Principles Compliance

**I. Fundamenta√ß√£o Cient√≠fica (NON-NEGOTIABLE)**
- ‚úÖ **PASS**: Spec references REFERENCIAS.md papers (Goodfellow 2016, Qi 2017, Redmon 2016)
- ‚úÖ **PASS**: Architecture choices documented for DECISIONS.md (FR-021, FR-022)
- ‚ö†Ô∏è **ACTION REQUIRED**: Must document final architecture choice in DECISIONS.md BEFORE implementation

**II. Rastreabilidade Total**
- ‚úÖ **PASS**: SpecKit workflow being followed (specify ‚Üí plan ‚Üí tasks ‚Üí implement)
- ‚úÖ **PASS**: Git branch 003-neural-networks created
- ‚ö†Ô∏è **ACTION REQUIRED**: Update TODO.md when phase complete, log metrics in logs/

**III. Desenvolvimento Incremental por Fases**
- ‚úÖ **PASS**: Phase 1 (sensor exploration) marked complete in TODO.md
- ‚úÖ **PASS**: Phase 2 builds on Phase 1 analysis (notebooks/sensor_analysis.ipynb)
- ‚úÖ **PASS**: Clear deliverables defined (SC-001 to SC-011)

**IV. Qualidade Senior**
- ‚úÖ **PASS**: Modular architecture planned (src/perception/)
- ‚úÖ **PASS**: Testing strategy defined (unit, integration, accuracy validation)
- ‚ö†Ô∏è **ACTION REQUIRED**: Implement tests achieving >80% coverage target

**V. Restri√ß√µes Disciplinares (NON-NEGOTIABLE)**
- ‚úÖ **PASS**: RNA requirement satisfied (MLP/CNN for LIDAR + CNN for camera)
- ‚úÖ **PASS**: No supervisor.py modification planned
- ‚úÖ **PASS**: No GPS in final demo (training only, per assumptions)
- ‚úÖ **PASS**: Scientific justification required for all choices (FR-021, FR-022)
- ‚ö†Ô∏è **FUTURE**: L√≥gica Fuzzy will be Phase 3 (not this phase)

**VI. Workflow SpecKit**
- ‚úÖ **PASS**: spec.md created with 29 FRs, 11 SCs, 3 user stories
- ‚úÖ **PASS**: No [NEEDS CLARIFICATION] markers in spec
- üîÑ **IN PROGRESS**: plan.md (this file) being generated
- ‚è≥ **NEXT**: research.md ‚Üí data-model.md ‚Üí tasks.md ‚Üí implement

### Constitution Compliance Summary

**Status**: ‚úÖ **APPROVED TO PROCEED**

All mandatory gates pass. Action items for implementation phase:
1. Document architecture decision in DECISIONS.md (MLP vs PointNet, YOLO vs SSD vs custom)
2. Update TODO.md Phase 2 checkboxes as tasks complete
3. Maintain >80% test coverage target
4. Log training metrics and model performance
5. Ensure Fuzzy Logic integration planned for Phase 3 handoff

## Project Structure

### Documentation (this feature)

```text
specs/003-neural-networks/
‚îú‚îÄ‚îÄ plan.md              # This file (/speckit.plan command output)
‚îú‚îÄ‚îÄ spec.md              # Feature specification (created by /speckit.specify)
‚îú‚îÄ‚îÄ research.md          # Phase 0 output (architecture & training research)
‚îú‚îÄ‚îÄ data-model.md        # Phase 1 output (entities and data structures)
‚îú‚îÄ‚îÄ quickstart.md        # Phase 1 output (training & inference guide)
‚îú‚îÄ‚îÄ contracts/           # Phase 1 output (API contracts for perception modules)
‚îÇ   ‚îú‚îÄ‚îÄ lidar_processor.py     # LIDARProcessor interface
‚îÇ   ‚îú‚îÄ‚îÄ cube_detector.py       # CubeDetector interface
‚îÇ   ‚îî‚îÄ‚îÄ perception_system.py   # PerceptionSystem integration API
‚îî‚îÄ‚îÄ tasks.md             # Phase 2 output (/speckit.tasks command - NOT created by /speckit.plan)
```

### Source Code (repository root)

```text
src/perception/                   # NEW: Neural network perception modules
‚îú‚îÄ‚îÄ lidar_processor.py            # LIDAR neural network wrapper
‚îú‚îÄ‚îÄ cube_detector.py              # Camera CNN wrapper
‚îú‚îÄ‚îÄ perception_system.py          # Integration layer (LIDAR + camera fusion)
‚îú‚îÄ‚îÄ training/                     # Training infrastructure
‚îÇ   ‚îú‚îÄ‚îÄ train_lidar.py           # LIDAR model training script
‚îÇ   ‚îú‚îÄ‚îÄ train_camera.py          # Camera model training script
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py           # Dataset loading utilities
‚îÇ   ‚îî‚îÄ‚îÄ augmentation.py          # Data augmentation functions
‚îî‚îÄ‚îÄ models/                       # Neural network architectures
    ‚îú‚îÄ‚îÄ lidar_net.py             # LIDAR network definition (MLP/PointNet/1D-CNN)
    ‚îî‚îÄ‚îÄ camera_net.py            # Camera network definition (Custom/YOLO/SSD/ResNet)

data/                             # NEW: Training/validation datasets
‚îú‚îÄ‚îÄ lidar/                        # LIDAR training data
‚îÇ   ‚îú‚îÄ‚îÄ scans/                   # Raw LIDAR scans (.npy files)
‚îÇ   ‚îî‚îÄ‚îÄ labels/                  # Obstacle annotations (.json)
‚îî‚îÄ‚îÄ camera/                       # Camera training data
    ‚îú‚îÄ‚îÄ images/                  # RGB images (.png files)
    ‚îî‚îÄ‚îÄ labels/                  # Cube bounding boxes + colors (.json)

models/                           # NEW: Trained model weights
‚îú‚îÄ‚îÄ lidar_net.pth                # Trained LIDAR model
‚îú‚îÄ‚îÄ camera_net.pth               # Trained camera model
‚îî‚îÄ‚îÄ metadata.json                # Model hyperparameters and metrics

notebooks/                        # EXISTING: Analysis notebooks
‚îú‚îÄ‚îÄ sensor_analysis.ipynb        # Phase 1 sensor exploration (already exists)
‚îú‚îÄ‚îÄ lidar_training.ipynb         # NEW: LIDAR model training experiments
‚îî‚îÄ‚îÄ camera_training.ipynb        # NEW: Camera model training experiments

IA_20252/controllers/youbot/      # EXISTING: Webots controller (to be modified)
‚îú‚îÄ‚îÄ youbot.py                    # Main controller - add perception integration
‚îú‚îÄ‚îÄ base.py, arm.py, gripper.py  # Existing control modules (unchanged)
‚îî‚îÄ‚îÄ test_controller.py           # Phase 1 tests (unchanged)

tests/perception/                 # NEW: Perception module tests
‚îú‚îÄ‚îÄ test_lidar_processor.py      # LIDAR unit tests
‚îú‚îÄ‚îÄ test_cube_detector.py        # Camera unit tests
‚îú‚îÄ‚îÄ test_perception_system.py    # Integration tests
‚îî‚îÄ‚îÄ test_data_loading.py         # Dataset utilities tests

logs/                             # NEW: Training and inference logs
‚îú‚îÄ‚îÄ lidar_training.log           # LIDAR training metrics
‚îú‚îÄ‚îÄ camera_training.log          # Camera training metrics
‚îî‚îÄ‚îÄ inference_performance.log    # Real-time performance benchmarks
```

**Structure Decision**: Single project structure chosen (Option 1). This is a robotics perception system that extends the existing youbot controller from Phase 1. All perception code lives in `src/perception/` to maintain modularity and separation from control logic (which will be Phase 3 fuzzy controller). Training infrastructure is separate from inference code to keep controller lightweight. Datasets and trained models are stored at repo root for easy access across notebooks and controllers.

**Integration Point**: Phase 1 delivered working `youbot.py` controller with sensor access. Phase 2 adds perception layer between sensors and controller. Phase 3 will add fuzzy logic between perception outputs and actuator commands.

## Complexity Tracking

> **No violations requiring justification**

Constitution Check passed all gates. No complexity violations detected. This phase follows standard incremental development:
- Builds on Phase 1 sensor exploration
- Uses mandatory technologies (RNA per discipline requirements)
- Maintains modular architecture
- Follows SpecKit workflow

No simpler alternatives rejected because requirements are minimal and architecture is straightforward perception pipeline.

---

## Post-Design Constitution Re-Check

*Re-evaluation after Phase 1 design artifacts complete*

### ‚úÖ Design Artifacts Generated

**Phase 0 (Research):**
- ‚úÖ research.md: Architecture decisions resolved (hybrid MLP+1D-CNN for LIDAR, custom CNN for camera)
- ‚úÖ Scientific justification: All choices backed by REFERENCIAS.md papers

**Phase 1 (Design):**
- ‚úÖ data-model.md: 7 core entities + 3 training entities defined
- ‚úÖ contracts/: API interfaces for LIDARProcessor, CubeDetector, PerceptionSystem
- ‚úÖ quickstart.md: Complete training and deployment guide

**Phase 1 (Agent Context):**
- ‚úÖ CLAUDE.md updated: PyTorch 2.0+, NumPy, SciPy, OpenCV, Matplotlib added to Active Technologies

### ‚úÖ Constitution Compliance Re-Check

**I. Fundamenta√ß√£o Cient√≠fica:**
- ‚úÖ research.md documents all architecture decisions with scientific references
- ‚úÖ Hybrid LIDAR: Goodfellow 2016 (Ch 12), Lenz 2015, LeCun 1998
- ‚úÖ Custom CNN: LeCun 1998, Krizhevsky 2012, Goodfellow 2016 (Ch 9, 11)
- ‚úÖ Training strategy: Qi 2017, Redmon 2016, Kingma & Ba 2014
- ‚ö†Ô∏è **ACTION REQUIRED**: Document final decisions in DECISIONS.md before implementation (DECIS√ÉO 016-017)

**II. Rastreabilidade:**
- ‚úÖ All design artifacts in specs/003-neural-networks/
- ‚úÖ Agent context updated (CLAUDE.md)
- ‚ö†Ô∏è **ACTION REQUIRED**: Update TODO.md when implementation starts

**III. Desenvolvimento Incremental:**
- ‚úÖ Phase 2 plan complete and detailed
- ‚úÖ Builds on Phase 1 (sensor_analysis.ipynb, arena_map.md)
- ‚úÖ Clear path to Phase 3 (fuzzy controller integration points documented)

**IV. Qualidade Senior:**
- ‚úÖ Modular architecture: src/perception/, data/, models/, tests/
- ‚úÖ API contracts define clean interfaces
- ‚úÖ Testing strategy in quickstart.md (pytest, accuracy validation, performance benchmarks)

**V. Restri√ß√µes Disciplinares:**
- ‚úÖ RNA requirement satisfied (2 neural networks: LIDAR + camera)
- ‚úÖ No GPS in final demo (only for training data collection)
- ‚úÖ Scientific justification for all architecture choices
- ‚úÖ No supervisor.py modifications planned

**VI. Workflow SpecKit:**
- ‚úÖ spec.md ‚Üí plan.md ‚Üí research.md ‚Üí data-model.md ‚Üí quickstart.md ‚Üí contracts/
- ‚è≥ **NEXT**: /speckit.tasks to generate implementation tasks
- ‚è≥ **THEN**: /speckit.implement to execute Phase 2

### Final Status: ‚úÖ **APPROVED FOR TASKS GENERATION**

All design gates passed. Ready to proceed to `/speckit.tasks` for granular task breakdown.

**Action Items Before Implementation:**
1. Run `/speckit.tasks` to generate tasks.md
2. Document architecture decisions in DECISIONS.md (DECIS√ÉO 016: LIDAR architecture, DECIS√ÉO 017: Camera architecture)
3. Update TODO.md Phase 2 section when implementation begins
4. Follow quickstart.md training guide during implementation
