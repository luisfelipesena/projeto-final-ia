Aplicação de Redes Neurais e Lógica Fuzzy em Robótica Móvel Autônoma com YouBot (LIDAR + Câmera RGB)

Introdução

Robôs móveis autônomos modernos combinam técnicas de inteligência artificial (como redes neurais) com sistemas de lógica fuzzy para navegar e interagir com o ambiente de forma inteligente. Neste relatório, focamos em um robô de arquitetura semelhante ao KUKA YouBot – uma base omnidirecional com braço manipulador e garra – equipado apenas com um sensor LIDAR 2D e uma câmera RGB. Abordaremos quatro aspectos principais: (1) métodos baseados em redes neurais (MLP ou CNN) para detecção de obstáculos via LIDAR e mapeamento do ambiente; (2) detecção e classificação de objetos coloridos (por exemplo, cubos) via câmera RGB, sem auxílio de GPS e considerando iluminação controlada; (3) estratégias com lógica fuzzy para tomada de decisão e controle de movimento, especialmente em ambientes com obstáculos e tarefas de manipulação (pegar e depositar objetos); e (4) estudos de caso e trabalhos relacionados, incluindo implementações em simuladores (como Webots) ou em robôs de arquitetura similar. As seções a seguir estão estruturadas de forma a discutir evidências técnicas e acadêmicas de cada tópico, incluindo tabelas comparativas, arquiteturas de solução e recomendações de implementação no contexto proposto.

1. Redes Neurais (MLP/CNN) para Detecção de Obstáculos e Mapeamento via LIDAR

Detecção de obstáculos com LIDAR usando MLP: Redes neurais feedforward (perceptron multilayer, MLP) já foram aplicadas com sucesso na classificação de obstáculos a partir de dados de sensores laser. Um exemplo clássico utilizou a nuvem de pontos de um LIDAR 3D (Velodyne) embarcado em veículo autônomo para segmentar e classificar obstáculos típicos urbanos. Após extrair características das nuvens, um MLP foi treinado para reconhecer classes como pedestres, veículos, troncos de árvore, postes e edifícios, alcançando 94,3% de acerto na validação ￼. Essa técnica comprovada demonstra que MLPs podem aprender a distinguir diferentes objetos no ambiente apenas pela assinatura geométrica dos dados LIDAR, dispensando modelos geométricos explícitos. Em robôs móveis, um MLP similar poderia ser treinado para distinguir entre “obstáculo baixo” (que o robô pode passar por cima) e “obstáculo alto” (precisa desviar) ou até identificar tipos de obstáculo (parede, pessoa, objeto manipulável etc.) a partir do perfil de distâncias do LIDAR.

Detecção de obstáculos com LIDAR usando CNN: As CNNs, mais conhecidas por aplicações em visão computacional, também vêm sendo empregadas para interpretar dados de LIDAR. Uma abordagem é converter os dados LIDAR em formas adequadas (por exemplo, projeções em 2D, imagens de range ou mapas de ocupação) e então aplicar CNNs para detecção de padrões. Por exemplo, no contexto de robôs de serviço (@Home), desenvolveu-se um sistema de people tracking chamado PeTra, em que uma CNN identifica em tempo real padrões de pernas humanas em scans de um LIDAR 2D montado próximo ao chão ￼ ￼. Essa CNN funciona sobre um mapa de ocupação gerado a partir do LIDAR, detectando pares de pernas (dois contornos próximos) que caracterizam a presença de um pedestre, mesmo em ambientes internos complexos. A solução provou ser eficaz ao permitir que o robô distinguisse pernas humanas de estruturas similares (pernas de mesa, cadeiras, vasos etc.), usando apenas o LIDAR ￼ ￼. Isso evidencia que CNNs conseguem extrair características espaciais dos dados LIDAR (mesmo 2D) para distinguir obstáculos específicos com precisão.

Mapeamento do ambiente com aprendizagem de máquina: O mapeamento é tradicionalmente feito via algoritmos de SLAM (Simultaneous Localization and Mapping) que não dependem diretamente de redes neurais, mas há pesquisas recentes integrando redes neurais para aprimorar mapas. Por exemplo, redes neurais convolucionais têm sido investigadas para construir mapas de ocupação dinâmicos com informações semânticas, combinando LIDAR e câmera ￼. Em um caso, utilizou-se sensor fusion (fusão de câmera e LIDAR) para melhorar a detecção de objetos e incorporar essas informações no mapa ocupacional ￼. Outro trabalho propôs um mapeamento probabilístico inspirado em cérebros biológicos, usando uma arquitetura multi-camadas CNN para inferir células ocupadas em um grid a partir de leituras LIDAR ruidosas ￼. Embora essas abordagens de Neural SLAM ainda estejam em desenvolvimento, elas apontam para cenários em que MLPs/CNNs aprendem a refinar mapas ou predizer áreas não vistas com base em experiência prévia. No contexto do YouBot (ambiente indoor controlado), a prática consolidada seria usar SLAM clássico (e.g. GMapping ou Cartographer para obter o mapa de ocupação) e possivelmente empregar redes neurais para tarefas complementares – como detecção de obstáculos dinâmicos sobre o mapa ou classificação de partes do mapa (por exemplo, reconhecer áreas navegáveis vs. proibidas). Resumindo, redes neurais podem apoiar o LIDAR tanto antes do mapeamento (filtrando/detectando obstáculos relevantes) quanto depois (interpretando o mapa gerado para tomadas de decisão de mais alto nível).

Fusionando LIDAR e redes neurais: Outra vertente a destacar é o uso combinado de LIDAR + rede neural end-to-end para navegação sem mapa prévio (mapless navigation). Um estudo recente (2025) treinou CNNs para fusão sensorial de imagens de câmera e profundidade LIDAR a fim de produzir comandos de desvio de obstáculos diretamente, sem construir mapa ￼ ￼. Essa abordagem imitativa aprendeu a partir de demonstrações humanas a dirigir o robô desviando de obstáculos, usando tanto informações geométricas (LIDAR) quanto visuais (RGB) em um modelo híbrido ￼ ￼. Os resultados mostraram 100% de sucesso em evitar obstáculos em tempo real com uma CNN fusão (Rede NetConEmb) rodando a bordo ￼ ￼. Embora tal técnica seja complexa e exija dataset robusto, ela demonstra que CNNs podem inferir navegação segura do robô a partir de dados LIDAR crus, integrando diferentes sensores. Para projetos práticos, uma alternativa mais simples e explicável é usar redes neurais para subtarefas (como detecção de obstáculos ou classificação semântica) dentro de uma arquitetura hierárquica de navegação.

Considerações para implementação: No projeto do YouBot, que dispõe de um LIDAR planar 2D (por exemplo, Hokuyo) fixado a certa altura, a detecção de obstáculos com técnicas neurais pode focar em interpretar o contorno 2D do laser. Uma opção robusta é treinar um classificador leve (MLP) com as leituras do LIDAR (ex.: 360 medidas de ângulo) para emitir probabilidades de “obstáculo à frente” vs “caminho livre” além de certa distância, complementando as simples regras de limiar. Já uma CNN 1D/2D poderia ser aplicada convertendo o scan LIDAR em uma imagem polar ou cartesiana (ex.: plot de alcance vs ângulo) e detectando padrões de obstáculos complexos (como a assinatura de uma esquina ou de uma passagem estreita). Em mapeamento, recomenda-se usar algoritmos testados de SLAM para obter o mapa de ocupação; as redes neurais entram para dar significado aos obstáculos detectados – por exemplo, distinguindo entre um obstáculo que é um móvel baixo (que poderia ser ignorado para navegação do base mas relevante para o braço) ou um humano (que requer desvio com maior distância por segurança). Em suma, técnicas baseadas em MLP/CNN para LIDAR já apresentam resultados comprovados em classificação de objetos ￼ e identificação de padrões (como pernas humanas) ￼, podendo ser adaptadas para melhorar a percepção do YouBot em termos de que obstáculo foi detectado e onde, indo além do mero “existe obstáculo a X metros”.

2. Detecção e Classificação de Objetos Coloridos com Câmera RGB (Sem GPS, Iluminação Controlada)

A identificação de objetos pelo atributo cor é uma tarefa comum em visão computacional robótica – por exemplo, localizar cubos coloridos para serem apreendidos pelo robô. Com iluminação controlada (pouca variação de luz, poucas sombras), métodos clássicos baseados em processamento de imagem podem ser bastante efetivos, competindo com abordagens de deep learning em simplicidade e velocidade.

Segmentação por cor (método clássico): Em cenários indoor com cores bem distintas, a técnica mais comprovada é usar o espaço de cor adequado (tipicamente HSV – Hue, Saturation, Value) e aplicar filtros limiares para segmentar pixels da cor de interesse. Essa abordagem aproveita o fato de que no espaço HSV a componente Hue aproxima o tom de cor independentemente da iluminação (que fica mais concentrada nas componentes Value e Saturation). Por exemplo, um algoritmo simples pode definir faixas HSV para cada cor de alvo (e.g. vermelho, verde, azul, amarelo) e gerar máscaras binárias indicando regiões de cada cor. Foi exatamente esse o método implementado em uma recente demonstração de pick-and-place de cubos coloridos com braço robótico: o código define intervalos HSV para vermelho, azul, amarelo etc., filtrando a imagem da câmera para detectar objetos dessas cores e localizar suas posições ￼. Após segmentar, usualmente aplica-se detecção de blobs/contornos para extrair cada região colorida e calcular propriedades (centroide em pixels, área, etc.). Sob iluminação controlada, esse pipeline alcança detecção em tempo real com alta confiabilidade, desde que as cores dos objetos sejam suficientemente distintas do fundo. Para melhorar a robustez, podem-se incorporar etapas como correção de balanço de branco, equalização de histograma (e.g. CLAHE) ou até iluminação auxiliar (LEDs) para reduzir sombras. Vale notar que variações de iluminação ainda podem afetar a cor percebida, mas por isso a condição de ambiente controlado é importante. Em casos de leve variação, técnicas adicionais podem incluir: usar espaços de cor normalizados (como c1c2c3), usar detecção adaptativa de cor (calibrar thresholds periodicamente com uma amostra do objeto alvo), ou ainda recorrer a iluminação multicolor (ex.: luzes RGB cíclicas) e filtros para realçar o contraste da cor desejada ￼. Ainda assim, em um experimento prático com câmera comum, a segmentação HSV se mostrou suficiente para distinguir blocos vermelhos, verdes e azuis para controle de um robô manipulador simples, com baixo custo computacional ￼.

Detecção baseada em CNN (redes profundas): Quando se deseja classificar e localizar objetos coloridos com maior invariância (por exemplo, diferentes condições de iluminação, ou presença de múltiplos objetos e cenários mais complexos), pode-se empregar técnicas de deep learning de visão, como redes do tipo YOLO, SSD ou Faster R-CNN. Essas redes são treinadas com conjuntos de imagens anotadas (com bounding boxes e classes) e aprendem a detectar objetos específicos. No contexto de cubos coloridos, seria relativamente simples montar um dataset de imagens das diferentes cores de cubos em variadas posições e treinar uma rede YOLO a identificá-los. Inclusive, já existem conjuntos de dados públicos e modelos pré-treinados para cubos coloridos – por exemplo, um dataset open-source com cubos de 13 cores diferentes, disponível no Roboflow Universe ￼, que serviu para treinar um modelo YOLOv5 capaz de detectar e classificar cubos azuis, verdes, vermelhos, amarelos etc. A vantagem de uma CNN de detecção é que ela pode ser mais robusta a mudanças sutis de iluminação e fundo, pois aprende características de alto nível (combinação de cor com forma, contorno, contexto). Além disso, consegue distinguir objetos mesmo que tenham cores parecidas, usando pistas de textura ou forma. Uma possível desvantagem é a necessidade de maior poder computacional e a latência, mas em 2025 mesmo embarcados acessíveis (ex.: Jetson Nano, Raspberry Pi 4) conseguem rodar um YOLO Tiny ou MobileNet SSD em ~real time para resoluções moderadas. Portanto, se o projeto exigir flexibilidade (por ex., os objetos coloridos podem variar de tamanho, ou a iluminação pode mudar um pouco), vale considerar um detector CNN customizado. Note que em ambientes altamente controlados, um pipeline híbrido também é possível: por exemplo, usar segmentação de cor para reduzir a área de busca e então aplicar uma pequena rede (ou template matching) para verificar se a região colorida corresponde de fato a um cubo alvo (evitando falsos positivos causados por reflexos, por exemplo). Essa combinação pode tirar proveito do baixo custo da segmentação e da capacidade discriminativa de uma rede treinada.

Sensibilidade à iluminação e ausência de GPS: A restrição de não usar GPS implica que a localização dos objetos deve ser inferida pelos sensores a bordo. Com uma única câmera RGB monocular, obter a distância absoluta de um objeto é desafiador – usualmente se requer visão estéreo ou algum outro sensor de profundidade. No caso do YouBot, podemos explorar o LIDAR para auxiliar: embora o LIDAR 2D não “veja” cor, ele pode detectar a presença de um obstáculo na direção que a câmera vê o cubo. Por exemplo, ao localizar um cubo vermelho na imagem, o robô pode orientar-se até alinhar o cubo ao centro da imagem e então avançar até o LIDAR registrar um obstáculo a uma curta distância naquele azimute – assim se estima a distância do cubo. Essa tática foi empregada em competições como RoboCup@Work: um objeto detectado pela câmera é aproximado com auxílio do rangefinder até entrar em alcance do braço. Alternativamente, sob iluminação controlada, pode-se colocar marcadores simples nos objetos (padrões conhecidos) para ajudar a estimar a pose – mas assumiremos que só a cor é usada mesmo.

A iluminação controlada facilita enormemente o uso de cor como pista confiável. Pesquisas indicam que a detecção de objetos tem sua acurácia degradada por variações de luz, sombras e oclusões ￼. Portanto, medidas como iluminação homogênea difusa no ambiente, redução de reflexos e ajuste de câmera (exposição fixa adequada) devem ser tomadas. Alguns projetos utilizam também luzes coloridas combinadas a filtros (por exemplo, iluminar com luz azul para realçar objetos azuis contra fundo vermelho, etc.) ￼, mas isso não costuma ser necessário se as cores dos objetos forem bem distintas do ambiente.

Exemplo prático: Um projeto acadêmico de 2018 desenvolveu um sistema de realidade aumentada para rastreamento de objeto colorido em vídeo de forma rápida e robusta ￼. A técnica apresentada permite segmentar um objeto de cor fornecida pelo usuário em um ambiente controlado, mesmo com ruídos e pequenas sombras, combinando informação de cor e forma ￼. Esse caso reforça que mesmo sem redes neurais pesadas, é possível atingir desempenho confiável usando os atributos intrínsecos do objeto (cor no caso) e processamento de imagem otimizado. Em nosso contexto, podemos implementar de forma eficiente: captura da imagem, conversão para HSV, filtro por faixa de cor, morphological opening/closing para limpar ruídos, detecção de contorno maior – obtendo assim a posição (u,v) do cubo na imagem. A partir disso, a lógica de controle (ver seção fuzzy) comandará o robô para centralizar e avançar.

Resumo das recomendações: Para detecção de cubos coloridos sem GPS, recomenda-se iniciar por algoritmos clássicos de visão dado o cenário favorável (iluminação controlada, cores distintas). Essa solução é determinística, explicável e de fácil depuração. Deve-se contudo testar a robustez a diferentes condições de luz; se houver instabilidade, pode-se avançar para uma solução de aprendizado profundo (treinar uma CNN leve tipo MobileNet ou YOLO-tiny para reconhecer os cubos). Em termos de implementação: a biblioteca OpenCV oferece funções prontas para conversão HSV e segmentação de cor, e frameworks como TensorFlow Lite ou ONNX Runtime permitem rodar detectores CNN em tempo real no hardware do robô. Independentemente do método, é vital calibrar no ambiente real – e.g., calibrar os limiares HSV tomando fotos do cubo no próprio cenário e ajustando para nenhuma detecção falsa do fundo. Com isso, o robô será capaz de localizar o objeto de interesse visualmente e discernir sua cor/classe, atendendo ao requisito de identificar, por exemplo, “cubos coloridos a serem coletados”.

3. Tomada de Decisão e Controle de Movimento Baseados em Lógica Fuzzy

A lógica fuzzy tem longa tradição em robótica móvel para enfrentar a natureza incerta e contínua dos sensores/atuadores. Diferente de controladores convencionais com limiares rígidos, um sistema fuzzy permite razão aproximada, combinando múltiplas entradas com regras heurísticas fornecidas por especialistas. No contexto do YouBot autônomo – navegando em meio a obstáculos e precisando pegar/depositar objetos – a lógica fuzzy pode atuar em diversos níveis: (a) no controle de navegação do robô móvel (evitar colisões, seguir para o alvo), (b) no controle do braço manipulador (alinhamento, agarrar soltar) e (c) na coordenação decisória entre tarefas (quando buscar um objeto, quando desviar, quando acionar o gripper etc.).

Fuzzy na navegação e desvio de obstáculos: Controladores fuzzy são amplamente usados para navegação autônoma por serem capazes de fundir leituras de múltiplos sensores e produzir comandos de movimento suaves. Um exemplo brasileiro clássico descreve um robô móvel com controle fuzzy que, lendo sensores de obstáculos, determina continuamente a direção segura para prosseguir ￼. As regras fuzzy nesse caso consideravam distâncias para obstáculos à frente e lateral, decidindo se o robô vai em frente, vira à direita, à esquerda, para ou dá ré conforme a situação ￼. Essa abordagem elimina a necessidade de um limiar fixo de distância para parar; ao invés disso, há conjuntos fuzzy como “obstáculo_perto”, “obstáculo_longe” e regras do tipo “SE obstáculo_perto_à_frente E livre_à_direita ENTÃO virar_direita”. O resultado é um comportamento de evitação de obstáculos suave e adaptativo, que foi demonstrado tanto em simulação quanto em protótipos reais desde os anos 1990. De fato, é reportado que o Sistema de Controle fuzzy “determina a direção mais adequada para um deslocamento seguro… controlando todos os atuadores do robô e determinando quando ele deve seguir em frente, desviar-se para direita/esquerda, parar ou retroceder” ￼, exatamente o conjunto de ações necessário para navegar corredores e contornar obstáculos sem colisão.

Uma estratégia comum é dividir a navegação em múltiplos comportamentos fuzzy e depois combiná-los (por blending ou arbiter). Por exemplo, pode-se ter um controlador fuzzy de seguir objetivo (move-to-goal) e outro de desviar obstáculos (avoid-obstacle). O primeiro considera ângulo e distância até o destino como entradas e produz comandos de velocidade linear/angular para avançar reto ao alvo ￼ ￼. O segundo considera distâncias laterais (e.g. de sonares ou LIDAR) e produz correções de direção para manter distância segura de paredes/obstáculos ￼. Em um estudo, esses controladores foram projetados com funções de pertinência gaussianas (7 níveis para erro angular, 7 para erro de distância) e regras do tipo “SE ângulo_muito_negativo ENTÃO velocidade_angular = grande_negativa” para girar rapidamente em direção ao alvo ￼ ￼. Similarmente, para parede: “SE distância_esq_pequena ENTÃO virar_direita_lenta” etc. Os resultados experimentais mostraram o robô seguindo paredes suavemente e atingindo pontos-alvo com erro mínimo ￼ ￼. A grande vantagem é que o fuzzy gera uma curva contínua de velocidades, evitando decisões bruscas. No caso do YouBot, que é omnidirecional, as regras podem ser adaptadas para controle holonômico (mas também dá para tratá-lo como diferencial decompondo vetores).

Fuzzy para manipulação de objetos (grasping): Além da navegação, a lógica fuzzy é útil na fase de manipulação – onde incertezas de alinhamento, atrito e força de preensão são críticas. Pesquisas desde os anos 1990 exploram fuzzy em controle de grippers e braços: R. Alberto et al. (1995) já demonstravam que fuzzy logic podia “imitar o comportamento humano” ao pegar objetos, obtendo melhores resultados tanto em condições conhecidas quanto desconhecidas, pois o sistema fuzzy conseguia decidir de forma adaptativa como fechar a garra dependendo do objeto detectado ￼ ￼. Por exemplo, pode-se usar entradas como força medida, deslizamento e posição dos dedos para um controlador fuzzy de força de aperto: SE (segurando_muito_fraco) E (deslizando) ENTÃO aumentar_força; SE (força_alta) E (objeto_firme) ENTÃO parar etc. Assim, mesmo sem um modelo preciso do objeto, a garra aperta “o suficiente”.

Um estudo comparativo em 2013 testou diferentes formas de pertinência (gaussiana, triangular, trapezoidal) em um controlador fuzzy para três dedos robóticos levantando um objeto ￼. Eles concluíram que funções Gauss resultaram em preensão mais firme e rápida que as demais, indicando que o ajuste fino das pertinências impacta desempenho ￼. Isso sugere que, ao projetar nosso controlador fuzzy de grasping, valeria a pena usar Gaussiana ou outra contínua suave para melhorar a resposta. De maneira geral, sistemas fuzzy podem gerenciar a transição entre fases: aproximação final, contato inicial, fechamento, elevação e soltura, de forma gradual. Por exemplo, SE (distância_objeto_pequena E alinhamento_bom) ENTÃO acionar_garra – garantindo que o robô só feche a garra quando o objeto estiver devidamente posicionado na frente. Outro fuzzy pode ajustar a velocidade do braço ao depositar o objeto, evitando solavancos: SE (objeto_nas_garra E perto_do_destino) ENTÃO movimento_lento.

Fusão de decisões (controle supervisório fuzzy): Uma aplicação elegante da lógica difusa é servir como supervisor de alto nível, decidindo qual comportamento ativar ou como ponderar comandos de diferentes módulos. Em arquiteturas behavior-based, por exemplo, ao invés de um arbiter binário, pode-se ter uma lógica fuzzy que leva em conta prioridades ou graus de urgência de cada comportamento. Imagine entradas fuzzy como “urgência_evitar” (derivada da distância do obstáculo mais próximo) e “urgência_perseguir_objetivo” (derivada da distância do cubo a pegar). Um conjunto de regras poderia então determinar as velocidades finais: SE (obstáculo_muito_perto) E (cubo_não_visto) ENTÃO priorizar_desvio, ou SE (cubo_visto) E (obstáculo_médio_distante) ENTÃO seguir_cubo. Esse blending fuzzy permite que o robô negocie situações complexas, como quando o objeto a ser coletado está atrás de um obstáculo – o sistema gradualmente alternaria do desvio para retomada de curso. Em suma, controladores fuzzy multi-entradas são adequados para coordenar navegação e manipulação: entradas podem incluir distância do alvo, alinhamento do cubo na câmera, distância de obstáculos à frente, se o robô está carregando objeto ou não, etc., e as saídas podem ser parâmetros de velocidade ou mesmo comandos discretos.

Exemplo aplicado (estudo de caso): Wu et al. (2013) implementaram um robô móvel de segurança com 4 rodas e braço 5-DOF, semelhante ao YouBot, que patrulha, detecta objetos no chão e os recolhe ￼ ￼. Para enfrentar erros de atrito do chão e imprecisão de câmera, eles desenvolveram controladores fuzzy tanto para as rodas quanto para a garra, afinados de acordo com especificações de hardware ￼. O fuzzy das rodas ajustava as velocidades motoras de modo que o robô parasse exatamente com o objeto dentro da área de alcance do braço (definiram uma “área pronta para agarrar” de 10 px no centro da imagem) ￼ ￼. Já o fuzzy do gripper considerava fatores como peso estimado do objeto e feedback visual para assegurar que a garra fechasse o suficiente sem deixar cair (corrigindo imprecisões causadas por inércia do robô) ￼. Os resultados mostraram que, sob diversas condições de atrito no piso, o sistema fuzzy reduziu erros de posicionamento e tornou os movimentos “precisos e eficientes”, mesmo usando sensores simples (uma câmera CCD e odometria) ￼. Isso ilustra bem o poder da abordagem: com lógica fuzzy, o robô compensou automaticamente situações como derrapagem (quando o chão escorregava, o fuzzy notava que o objeto não estava na posição esperada na câmera e comandava ajustes nas rodas) e frenagem antecipada (para não ultrapassar o alvo devido à inércia). Em síntese, a lógica fuzzy permitiu ao robô lidar com incertezas físicas e assegurar o sucesso do agarramento, sem reprogramação manual para cada condição.

Diretrizes de projeto fuzzy para o YouBot: Com base nessas referências, podemos delinear uma possível arquitetura de controle fuzzy no projeto:
	•	Usar dois controladores fuzzy acoplados para navegação: um para alinhar em direção ao objeto alvo (entradas: ângulo do cubo na imagem, distância estimada do cubo; saídas: vel. linear e angular do base) e outro para evitar colisões (entradas: distâncias LIDAR frontal e laterais; saída: incremento de ângulo ou fator de redução de velocidade). A composição pode ser via regras fuzzy combinadas ou blending linear controlado por fuzzy.
	•	Um controlador fuzzy no manipulador: entradas podem ser erro de alinhamento final do cubo (na câmera, ou sensor no braço), talvez distância do braço até a posição de pegar (via encoders), e saída seria velocidade do movimento do braço e comando de fechar garra graduais. Regras como: SE (alinhamento_bom E distância_curta) ENTÃO fechar_garra_devagar, SE (objeto_agarrado_firme) ENTÃO levantar_braço, etc., garantindo que cada etapa só ocorre quando critérios suaves são satisfeitos.
	•	Regras fuzzy de transição de estado: embora muitas arquiteturas de robótica usem máquinas de estado discretas (e.g., STATE=“procurando”, “movendo até cubo”, “agarra”, “carregando”, “depositando”), é possível usar um fuzzy state controller onde múltiplos estados têm graus de ativação. Por exemplo, grau_busca diminui conforme o grau_rastreamento_objeto aumenta. Isso pode evitar oscilações e histerese, tornando o comportamento do robô mais gradativo. Ainda que seja possível, talvez manter estados discretos e usar fuzzy apenas para dentro de cada estado seja mais simples e justificável academicamente.

Em conclusão, a lógica fuzzy oferece uma camada de tomada de decisão tolerante a ruídos e imprecisões, essencial em ambientes reais. Conforme evidenciado em diversos trabalhos, controladores fuzzy bem projetados permitem que um robô móvel navegue desviando obstáculos com suavidade ￼, alcance alvos móveis ou estacionários ￼ ￼, e manipule objetos incertos de forma robusta ￼. No projeto YouBot proposto, a aplicação integrada dessas técnicas fuzzy garantirá que as leituras imperfeitas do LIDAR e da câmera sejam convertidas em comandos de movimento confiáveis – por exemplo, evitando que um cubo perdido em sombra ou um falso positivo de obstáculo cause comportamentos erráticos. O resultado esperado é um robô que se desloca de forma inteligente e suave, combinando objetivos de navegação e manipulação sem necessidade de sintonia manual exaustiva, já que o próprio sistema fuzzy acomoda a variabilidade do mundo real.

4. Casos de Sucesso e Estudos Similares (Simulação Webots e Robôs Equivalentes)

É valioso mencionar exemplos documentados de implementação das ideias acima, seja em plataformas similares ao YouBot, seja em ambientes simulados como Webots, que permitem testar esses conceitos integrados.

Robô YouBot em competição (RoboCup@Work): A equipe RED (Rússia) reportou seus resultados na RoboCup@Work 2018 usando um KUKA youBot customizado com câmera RGB-D e LIDAR como plataforma principal ￼. Eles integraram o ROS Navigation Stack (baseado em LIDAR Hokuyo para SLAM e planejamento global) com módulos de visão computacional para detectar objetos industriais a serem manipulados. Em seu Team Description Paper, descrevem ter substituído a pinça original por uma garra Festo adaptada, e usaram uma câmera Intel RealSense SR300 montada na frente do robô para identificar peças, ignorar sombras e auxiliar na preensão ￼ ￼. O sucesso do time RED demonstra a viabilidade de uma arquitetura similar: um youBot omnidirecional navegando autonomamente (sem GPS, posicionamento derivado de SLAM) e usando visão para encontrar objetos coloridos ou etiquetados. Embora não entrem em detalhes de fuzzy ou redes neurais, o sistema deles contava com algoritmos robustos de controle e planejamento (A* para trajeto global, planner local holonômico) e visão para detecção e localização de peças, com o robô conseguindo realizar tarefas de pegar e entregar objetos em um ambiente de fábrica. Esse caso reforça a pertinência de usar o youBot com sensores limitados (LIDAR+camera) e obter desempenho satisfatório ao combinar métodos clássicos e inteligência artificial.

Simulação no Webots com controle fuzzy: Diversos trabalhos acadêmicos utilizam o simulador Webots para prototipar controladores fuzzy e de visão antes de partir para o robô real. Por exemplo, pesquisadores implementaram um robô móvel virtual com câmeras e LIDAR no Webots e testaram diferentes estratégias de desvio de obstáculo: um dos experimentos comparou algoritmo de histograma de vetores (VFH) com um controle fuzzy multicritério para evitar obstáculos – com o fuzzy demonstrando rota mais suave e eficiente em manter distância segura ￼. Infelizmente não temos a referência exata do artigo via busca, mas sabe-se que Webots possui tutoriais de robô evitando obstáculos via lógica fuzzy. Além disso, há estudos de segue-linha e segue-pessoa simulados em Webots com fuzzy e visão: um artigo apresentou um robô seguidor de linha no Webots que pré-processa imagem (dilatação/erosão) e então usa regras fuzzy para alinhar o eixo do robô com a linha detectada ￼. Isso é análogo ao que faríamos com o YouBot para alinhar com o cubo colorido detectado – a diferença é que em vez de uma linha contínua, seria o blob colorido, mas o princípio (girar para reduzir erro de ângulo, avançar para reduzir erro de posição) permanece.

Deep Learning integrado em YouBot: Em 2021-2023 surgiram pesquisas combinando deep learning e robótica móvel manipuladora. Por exemplo, Janos et al. (2025) propuseram um sistema de detecção de cones de tráfego para um manipulador móvel industrial ￼. Embora focado em cones, o robô descrito tinha uma arquitetura de fusão câmera-LIDAR similar: usava YOLO para detecção e uma manipulação autônoma para pegar e posicionar cones ￼. Isso espelha nosso cenário de cubos coloridos, apenas trocando o objeto. Outro trabalho, Qian et al. (2025), revisou métodos de fusão multi-sensor para robôs autônomos e ilustrou arquiteturas onde CNNs processam simultaneamente imagens e nuvens LIDAR para obter percepções robustas ￼. Eles classificam arquiteturas em fusão antecipada (dados brutos combinados antes da rede), fusão intermediária e tardia. Essa perspectiva de alto nível pode guiar nosso design se quisermos experimentar aprendizado de máquina para fusão dos sensores do YouBot futuramente.

Controle por reforço e fuzzy: Vale citar um estudo que combinou Aprendizado por Reforço Profundo com Fuzzy em um robô humanoide para otimizar o ato de agarrar objetos ￼. Eles usaram deep reinforcement learning para ajustar parâmetros de um controlador fuzzy de dois estágios para preensão. O algoritmo aprendia a calibrar as funções de pertinência conforme tentativas de pegar objetos. Isso resultou em um sistema adaptativo que melhorou o desempenho do fuzzy ao longo do tempo. Embora complexo, esse trabalho sinaliza que a união de fuzzy + aprendizado é promissora – no contexto do YouBot, poderíamos imaginar no futuro um módulo de RL ajustando regras fuzzy de navegação com base em experiências de sucesso/fracasso em missões de pegar cubos.

Robótica móvel onipresente (exemplos adicionais): Projetos como o CaRINA (USP, Brasil) criaram plataformas móveis com manipuladores e usaram intensivamente técnicas AI. No caso do CaRINA 2 (veículo autônomo com braço), eles empregaram LIDAR 3D e câmeras para detecção de obstáculos e pessoas ￼, e integraram controle autônomo de navegação. Enquanto o CaRINA é um carro de porte maior, as lições se aplicam: a fusão de sensores permitiu navegação sem mapa prévio detalhado e sem localização global precisa, confiando apenas em referências locais (GPS apenas aproximado) ￼. Isso reforça a ideia de que nosso YouBot pode operar indo buscar objetos em um ambiente conhecido sem necessidade de GPS, desde que tenha um mapa ou consiga se localizar pelos próprios sensores.

Comparativo de técnicas (MLP/CNN vs Fuzzy): Podemos resumir as abordagens discutidas em uma tabela para destacar como elas se complementam:

Tarefa/Componente	Método baseado em Aprendizado (MLP/CNN)	Método baseado em Lógica Fuzzy
Detecção de Obstáculo (LIDAR)	MLP classifica tipos de obstáculo (pedestre, veículo, etc.) a partir da nuvem ￼.CNN identifica padrão específico (ex: pernas humanas) no scan 2D ￼.	Regras fuzzy combinam distâncias de múltiplos sensores para decidir desvio seguro ￼. Ex.: “SE obstáculo_muito_perto ENTÃO reduzir velocidade e virar”.
Mapeamento e Localização	CNNs para Neural SLAM (ocupancy grids com inferência) – ainda experimental ￼.Visão+LIDAR fusion para enriquecer mapas semânticos.	Algoritmos clássicos de SLAM (não fuzzy) fornecem mapa. Fuzzy pode ajudar na localização aproximada usando sensores (e.g., combinação fuzzy de leitura de marco visual + odometria).
Detecção de Objeto Colorido (Visão)	CNN de detecção de objetos (YOLO/SSD) reconhece e localiza cubos de cada cor – robusto a variações, requer dataset e computação.	Segmentação de cor HSV e rastreamento de blob – simples, rápida, exige iluminação estável ￼. Fuzzy pode auxiliar pós-processamento (ex.: “SE blob_pequeno E baixa_confiança ENTÃO ignorar”).
Controle de Navegação	Redes neurais recorrentes podem aprender a controlar o robô de forma end-to-end (da leitura LIDAR+camera aos comandos) ￼ – requer treino extenso, difícil de depurar.	Controlador Fuzzy determina velocidades de translação/rotação a partir de erros de ângulo e distância ￼ ￼. Fácil de ajustar regras e compreensão intuitiva do comportamento.
Controle de Manipulação	Algoritmos de aprendizado (imitação ou reforço) podem otimizar trajetória do braço e força de garra – usualmente offline (treino em simulação).	Controle Fuzzy de preensão considera força e posição para fechar garra adequadamente ￼ ￼. Fuzzy no braço lida com incerteza de alinhamento (ex.: ajusta punho se objeto um pouco fora do centro).
Decisão e Coordenação	Planejadores baseados em árvore de decisão aprendida ou redes neurais de alto nível (ex.: redes neurais neurais profundas para seleção de ação) – difícil garantir todas regras de segurança.	Sistema Fuzzy supervisório pondera objetivos conflitantes (navegar vs evitar vs agarrar) de forma contínua. Regras definidas por especialista asseguram prioridades (ex.: evitar colisão sempre tem peso alto).

A tabela acima ilustra que técnicas de redes neurais e lógica fuzzy não competem, mas se complementam: as primeiras são ótimas para percepção (detectar e classificar a partir de dados brutos) e eventualmente controle aprendido em cenários bem modelados, enquanto a segunda fornece um arcabouço de decisão linguística para integrar percepções e acionar comandos de forma segura e explicável. No projeto do YouBot, podemos combinar o melhor de ambos – por exemplo, usar um MLP para identificar que tipo de obstáculo o LIDAR vê (humano vs objeto inanimado) e então, com base nisso, o fuzzy decide manter maior distância caso seja humano. Similarmente, uma CNN poderia identificar a cor do cubo, e via fuzzy o robô decide em qual local de depósito correspondente (ex.: cubo azul vai para caixa azul, usando uma regra SE cor=azul ENTÃO destino = zona A).

Conclusão desta pesquisa: A aplicação de MLP/CNN e lógica fuzzy em robótica móvel mostrou-se não apenas teoricamente viável mas comprovada em múltiplos estudos. Redes neurais proporcionam ao robô percepção avançada, extraindo informação de alto nível de sensores (como reconhecer classes de obstáculos via LIDAR ￼ ou identificar um objeto alvo em visão mesmo com ruídos ￼). Já a lógica fuzzy fornece a inteligência de controle, replicando decisões humanas de condução e manipulação – navegar evitando perigos e cumprindo objetivos ￼ – de maneira contínua e robusta a imprecisões. Casos reais, tanto em competição (youBot no RoboCup) quanto em laboratório (robôs similares implementados em artigos), reforçam que a sinergia dessas abordagens permite atingir autonomia sofisticada mesmo sem sensores caros ou posicionamento global. Em ambientes simulados como Webots, pode-se já experimentar e refinar as regras fuzzy e os detectores de visão, facilitando a transferência para o robô físico posteriormente.

Para o projeto específico (YouBot com LIDAR e câmera RGB coletando cubos coloridos), as recomendações finais são: usar o LIDAR com SLAM tradicional para navegação básica, complementando com um MLP/CNN para caracterização de obstáculos importantes; utilizar visão computacional baseada em cor para detectar os cubos, recorrendo a CNN apenas se necessário para robustez extra; e implementar um sistema de controle fuzzy hierárquico que gerencie o deslocamento do robô, a evitação de obstáculos e as ações do manipulador de forma integrada. Assim, espera-se um desempenho ótimo no desafio de pegar e depositar objetos em ambiente não estruturado, com o robô conseguindo percorrer o espaço evitando colisões, identificar corretamente os objetos de interesse e manipulá-los com segurança e eficácia.

Referências Selecionadas:
	•	Habermann, D. et al. (2013). Detecção e Classificação de Objetos com uso de Sensor Laser para Veículos Autônomos Terrestres. Anais do SIGE – Exemplo de uso de MLP com LIDAR 3D para classificar obstáculos urbanos ￼.
	•	Merino, J. et al. (2019). People Detection and Tracking Using LiDAR Sensors. Sensors, 19(3) – Descreve o PeTra (People Tracker) usando CNN em LIDAR 2D para detectar pernas humanas ￼ ￼.
	•	Oliveira, W. (2018). Detecção e rastreamento em tempo real de objetos coloridos. TCC UFAL – Demonstra segmentação de objetos coloridos em ambiente controlado, discutindo desafios de iluminação ￼.
	•	Gopalakrishnan, V. (2024). Color Recognition and Robotic Arm Coordination. Medium – Implementação prática de identificação de cubos por cor (HSV) e coordenação com braço MyCobot ￼.
	•	Rufino, F.A.O. & Riul, J.A. (2009). Robô móvel autônomo com controle fuzzy. Revista Ci. Exatas UNITAU – Projeto pioneiro de navegação fuzzy, com robô desviando obstáculos e seguindo objetivos via regras difusas ￼.
	•	Wu, C.H. et al. (2013). Fuzzy Control of Target Approaching and Object-Grabbing for a Mobile Robot. J. Adv. Comp. Intell. & Int. Informatics 17(2) – Relato completo de um robô móvel com braço usando controladores fuzzy nas rodas e garra, validando robustez em condições reais ￼.
	•	RED Team (2018). RoboCup@Work – Team Description Paper (ITMO University) – Descreve hardware/software de um KUKA youBot com LIDAR e câmera vencendo desafios de fábrica (exemplo de caso real aplicado) ￼.
	•	Yan, C. et al. (2022). Mapless navigation with safety-enhanced imitation learning. IEEE T. Ind. Electronics 70(7) – Exemplo de uso de CNN end-to-end para navegação baseada em LIDAR (segurança aprimorada) ￼.
	•	János, H. et al. (2025). YOLO-Based Object and Keypoint Detection for Autonomous Traffic Cone Placement. Int. J. of Robotics – Apresenta manipulador móvel autônomo colocando cones, com fusões de câmera-LIDAR e YOLO, análogo ao pick-place de cubos (pré-print) ￼.

(Outras referências e detalhes no texto principal acima.)