{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensor Exploration - Phase 1.3\n",
    "\n",
    "**Feature**: 002-sensor-exploration  \n",
    "**Author**: Luis Felipe Cordeiro Sena  \n",
    "**Date**: 2025-11-21  \n",
    "\n",
    "## Objectives\n",
    "\n",
    "- **US3 (FR-014 to FR-019)**: LIDAR data analysis and visualization\n",
    "- **US4 (FR-020 to FR-026)**: Camera RGB analysis and color detection\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Webots R2023b running with `IA_20252.wbt` loaded\n",
    "- Python venv active with dependencies (numpy, matplotlib, opencv, scipy)\n",
    "- YouBot controller integrated with this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "# Enable inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# Add controller path\n",
    "CONTROLLER_PATH = Path().absolute().parent / 'IA_20252' / 'controllers' / 'youbot'\n",
    "sys.path.insert(0, str(CONTROLLER_PATH))\n",
    "\n",
    "print(f\"âœ… Setup complete\")\n",
    "print(f\"ðŸ“‚ Controller path: {CONTROLLER_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LIDAR Analysis (FR-014 to FR-019)\n",
    "\n",
    "### 1.1 Specifications (FR-015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_lidar_specifications(lidar):\n",
    "    \"\"\"Extract and document LIDAR sensor specifications (FR-015).\"\"\"\n",
    "    specs = {\n",
    "        'horizontal_resolution': lidar.getHorizontalResolution(),\n",
    "        'num_layers': lidar.getNumberOfLayers(),\n",
    "        'fov': lidar.getFov(),\n",
    "        'vertical_fov': lidar.getVerticalFov() if hasattr(lidar, 'getVerticalFov') else 0.0,\n",
    "        'min_range': lidar.getMinRange(),\n",
    "        'max_range': lidar.getMaxRange(),\n",
    "    }\n",
    "    specs['angular_resolution'] = specs['fov'] / (specs['horizontal_resolution'] - 1)\n",
    "    return specs\n",
    "\n",
    "# NOTE: Execute this after initializing robot in Webots\n",
    "# from controller import Robot\n",
    "# robot = Robot()\n",
    "# lidar = robot.getDevice(\"lidar\")\n",
    "# lidar.enable(int(robot.getBasicTimeStep()))\n",
    "# robot.step(int(robot.getBasicTimeStep()))\n",
    "# lidar_specs = capture_lidar_specifications(lidar)\n",
    "# print(\"LIDAR Specifications:\", lidar_specs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Capture (FR-014, FR-016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_lidar_scan(lidar):\n",
    "    \"\"\"Capture single LIDAR scan (FR-014).\"\"\"\n",
    "    range_image = lidar.getRangeImage()\n",
    "    return np.array(range_image, dtype=np.float64)\n",
    "\n",
    "def analyze_lidar_ranges(scans, specs):\n",
    "    \"\"\"Analyze LIDAR detection range capabilities (FR-016).\"\"\"\n",
    "    all_scans = np.concatenate(scans)\n",
    "    valid_readings = all_scans[np.isfinite(all_scans)]\n",
    "    \n",
    "    return {\n",
    "        'observed_min_range': float(np.min(valid_readings)) if len(valid_readings) > 0 else 0.0,\n",
    "        'observed_max_range': float(np.max(valid_readings)) if len(valid_readings) > 0 else 0.0,\n",
    "        'mean_range': float(np.mean(valid_readings)) if len(valid_readings) > 0 else 0.0,\n",
    "        'std_range': float(np.std(valid_readings)) if len(valid_readings) > 0 else 0.0,\n",
    "        'valid_readings_pct': float(len(valid_readings) / len(all_scans) * 100) if len(all_scans) > 0 else 0.0,\n",
    "    }\n",
    "\n",
    "# Example execution:\n",
    "# scans = [capture_lidar_scan(lidar) for _ in range(20)]\n",
    "# range_analysis = analyze_lidar_ranges(scans, lidar_specs)\n",
    "# print(\"Range Analysis:\", range_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Visualization (FR-017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_lidar_polar(range_image, fov, max_range, title=\"LIDAR Scan\"):\n",
    "    \"\"\"Generate polar plot of LIDAR scan (FR-017).\"\"\"\n",
    "    num_points = len(range_image)\n",
    "    angles = np.linspace(-fov/2, fov/2, num_points)\n",
    "    \n",
    "    # Replace inf with max_range for visualization\n",
    "    distances = np.where(np.isinf(range_image), max_range, range_image)\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111, projection='polar')\n",
    "    \n",
    "    # Plot scan points\n",
    "    scatter = ax.scatter(angles, distances, c=distances, cmap='RdYlGn_r', s=20, alpha=0.6)\n",
    "    \n",
    "    # Colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=ax, pad=0.1)\n",
    "    cbar.set_label('Distance (m)', rotation=270, labelpad=20)\n",
    "    \n",
    "    # Labels\n",
    "    ax.set_theta_zero_location('N')\n",
    "    ax.set_title(title, va='bottom', fontsize=14, pad=20)\n",
    "    ax.set_ylim(0, max_range)\n",
    "    ax.grid(True)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Example:\n",
    "# fig = visualize_lidar_polar(scans[0], lidar_specs['fov'], lidar_specs['max_range'])\n",
    "# plt.savefig('../media/lidar_scans/lidar_scan_example.png', dpi=150, bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Obstacle Detection (FR-018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_obstacles_lidar(range_image, fov, threshold=1.0):\n",
    "    \"\"\"Detect obstacles from LIDAR scan using distance threshold (FR-018).\"\"\"\n",
    "    num_points = len(range_image)\n",
    "    angles = np.linspace(-fov/2, fov/2, num_points)\n",
    "    \n",
    "    obstacles = []\n",
    "    for i, (distance, angle) in enumerate(zip(range_image, angles)):\n",
    "        if np.isfinite(distance) and distance < threshold:\n",
    "            # Calculate confidence based on neighboring points\n",
    "            neighbors = range_image[max(0, i-2):min(num_points, i+3)]\n",
    "            valid_neighbors = neighbors[np.isfinite(neighbors)]\n",
    "            confidence = len(valid_neighbors) / 5.0  # Up to 5 neighbors\n",
    "            \n",
    "            obstacles.append({\n",
    "                'distance': float(distance),\n",
    "                'angle': float(angle),\n",
    "                'index': i,\n",
    "                'confidence': confidence,\n",
    "            })\n",
    "    \n",
    "    return obstacles\n",
    "\n",
    "# Example:\n",
    "# obstacles = identify_obstacles_lidar(scans[0], lidar_specs['fov'], threshold=1.0)\n",
    "# print(f\"Detected {len(obstacles)} obstacles within 1.0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Camera Analysis (FR-020 to FR-026)\n",
    "\n",
    "### 2.1 Specifications (FR-021, FR-022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_camera_specifications(camera):\n",
    "    \"\"\"Extract and document camera sensor specifications (FR-021, FR-022).\"\"\"\n",
    "    specs = {\n",
    "        'width': camera.getWidth(),\n",
    "        'height': camera.getHeight(),\n",
    "        'fov': camera.getFov(),\n",
    "        'channels': 4,  # BGRA\n",
    "        'has_recognition': camera.hasRecognition() if hasattr(camera, 'hasRecognition') else False,\n",
    "    }\n",
    "    return specs\n",
    "\n",
    "def measure_camera_fps(camera, duration_sec=10):\n",
    "    \"\"\"Measure camera frame rate (FR-022).\"\"\"\n",
    "    # NOTE: Requires robot.step() calls in loop\n",
    "    # frame_count = 0\n",
    "    # start_time = robot.getTime()\n",
    "    # while robot.getTime() - start_time < duration_sec:\n",
    "    #     robot.step(time_step)\n",
    "    #     camera.getImage()  # Trigger frame capture\n",
    "    #     frame_count += 1\n",
    "    # fps = frame_count / duration_sec\n",
    "    # return fps\n",
    "    pass  # Placeholder\n",
    "\n",
    "# Example:\n",
    "# camera = robot.getDevice(\"camera\")\n",
    "# camera.enable(time_step)\n",
    "# robot.step(time_step)\n",
    "# camera_specs = capture_camera_specifications(camera)\n",
    "# print(\"Camera Specifications:\", camera_specs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Frame Capture (FR-020, FR-023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_camera_frame(camera):\n",
    "    \"\"\"Capture single camera frame and convert to RGB (FR-020).\"\"\"\n",
    "    image_data = camera.getImage()\n",
    "    if image_data is None:\n",
    "        return None\n",
    "    \n",
    "    width = camera.getWidth()\n",
    "    height = camera.getHeight()\n",
    "    \n",
    "    # Convert BGRA to RGB\n",
    "    img = np.frombuffer(image_data, np.uint8).reshape((height, width, 4))\n",
    "    img_rgb = img[:, :, [2, 1, 0]]  # BGR to RGB\n",
    "    \n",
    "    return img_rgb\n",
    "\n",
    "def save_example_images(camera, robot, colors=['green', 'blue', 'red'], output_dir='../media/cube_examples'):\n",
    "    \"\"\"Capture and save example images of colored cubes (FR-023).\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    saved_files = {}\n",
    "    \n",
    "    # NOTE: Manual positioning required to capture each color\n",
    "    # For automation, use recognition API or position robot near known cubes\n",
    "    \n",
    "    print(\"âš ï¸ save_example_images requires manual execution:\")\n",
    "    print(\"   1. Position robot near green cube\")\n",
    "    print(\"   2. Run: frame = capture_camera_frame(camera)\")\n",
    "    print(\"   3. Save: cv2.imwrite('green_cube.png', cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\")\n",
    "    print(\"   4. Repeat for blue and red cubes\")\n",
    "    \n",
    "    return saved_files\n",
    "\n",
    "# Example:\n",
    "# frame = capture_camera_frame(camera)\n",
    "# plt.imshow(frame)\n",
    "# plt.title(\"Camera Frame\")\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Color Detection (FR-024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_color_threshold(image_rgb, method='hsv'):\n",
    "    \"\"\"Classify cube color using threshold method (FR-024).\"\"\"\n",
    "    # HSV thresholds (to be calibrated with real images)\n",
    "    HSV_RANGES = {\n",
    "        'green': ((40, 50, 50), (80, 255, 255)),\n",
    "        'blue': ((90, 50, 50), (130, 255, 255)),\n",
    "        'red_low': ((0, 50, 50), (10, 255, 255)),   # Red wraps around\n",
    "        'red_high': ((170, 50, 50), (180, 255, 255)),\n",
    "    }\n",
    "    \n",
    "    if method == 'hsv':\n",
    "        # Convert to HSV\n",
    "        img_hsv = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2HSV)\n",
    "        \n",
    "        # Count pixels for each color\n",
    "        pixel_counts = {}\n",
    "        \n",
    "        for color, (lower, upper) in HSV_RANGES.items():\n",
    "            if color.startswith('red'):\n",
    "                continue  # Handle red separately\n",
    "            \n",
    "            mask = cv2.inRange(img_hsv, np.array(lower), np.array(upper))\n",
    "            pixel_counts[color] = int(np.sum(mask > 0))\n",
    "        \n",
    "        # Red color (combine low and high ranges)\n",
    "        mask_red_low = cv2.inRange(img_hsv, np.array(HSV_RANGES['red_low'][0]), np.array(HSV_RANGES['red_low'][1]))\n",
    "        mask_red_high = cv2.inRange(img_hsv, np.array(HSV_RANGES['red_high'][0]), np.array(HSV_RANGES['red_high'][1]))\n",
    "        pixel_counts['red'] = int(np.sum(mask_red_low > 0) + np.sum(mask_red_high > 0))\n",
    "        \n",
    "        # Determine detected color\n",
    "        total_pixels = image_rgb.shape[0] * image_rgb.shape[1]\n",
    "        max_color = max(pixel_counts, key=pixel_counts.get)\n",
    "        confidence = pixel_counts[max_color] / total_pixels\n",
    "        \n",
    "        detected_color = max_color if confidence > 0.05 else 'unknown'  # 5% threshold\n",
    "        \n",
    "        return {\n",
    "            'detected_color': detected_color,\n",
    "            'confidence': confidence,\n",
    "            'pixel_counts': pixel_counts,\n",
    "            'method': method,\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(f\"Method '{method}' not implemented\")\n",
    "\n",
    "# Example:\n",
    "# result = detect_color_threshold(frame, method='hsv')\n",
    "# print(f\"Detected: {result['detected_color']} (confidence: {result['confidence']:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Accuracy Evaluation (FR-025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_color_detection_accuracy(test_images):\n",
    "    \"\"\"Evaluate threshold method on labeled test set (FR-025).\"\"\"\n",
    "    if len(test_images) == 0:\n",
    "        return {\n",
    "            'overall_accuracy': 0.0,\n",
    "            'per_color_accuracy': {},\n",
    "            'confusion_matrix': np.zeros((3, 3)),\n",
    "            'num_samples': 0,\n",
    "        }\n",
    "    \n",
    "    colors = ['green', 'blue', 'red']\n",
    "    confusion = np.zeros((3, 3), dtype=int)\n",
    "    \n",
    "    correct = 0\n",
    "    for img, ground_truth in test_images:\n",
    "        result = detect_color_threshold(img, method='hsv')\n",
    "        predicted = result['detected_color']\n",
    "        \n",
    "        if predicted == ground_truth:\n",
    "            correct += 1\n",
    "        \n",
    "        # Update confusion matrix\n",
    "        if ground_truth in colors and predicted in colors:\n",
    "            gt_idx = colors.index(ground_truth)\n",
    "            pred_idx = colors.index(predicted)\n",
    "            confusion[gt_idx, pred_idx] += 1\n",
    "    \n",
    "    overall_accuracy = correct / len(test_images)\n",
    "    \n",
    "    # Per-color accuracy\n",
    "    per_color_accuracy = {}\n",
    "    for i, color in enumerate(colors):\n",
    "        total_color = np.sum(confusion[i, :])\n",
    "        correct_color = confusion[i, i]\n",
    "        per_color_accuracy[color] = correct_color / total_color if total_color > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'overall_accuracy': overall_accuracy,\n",
    "        'per_color_accuracy': per_color_accuracy,\n",
    "        'confusion_matrix': confusion,\n",
    "        'num_samples': len(test_images),\n",
    "    }\n",
    "\n",
    "# Example:\n",
    "# test_images = [(frame1, 'green'), (frame2, 'blue'), (frame3, 'red')]\n",
    "# accuracy = evaluate_color_detection_accuracy(test_images)\n",
    "# print(f\"Overall Accuracy: {accuracy['overall_accuracy']:.1%}\")\n",
    "# print(f\"Per-color: {accuracy['per_color_accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Results Summary\n",
    "\n",
    "### 3.1 LIDAR Findings\n",
    "\n",
    "**Specifications:**\n",
    "- Horizontal resolution: [VALUE] points\n",
    "- FOV: [VALUE] rad\n",
    "- Max range: [VALUE] m\n",
    "\n",
    "**Range Analysis:**\n",
    "- Observed min: [VALUE] m\n",
    "- Observed max: [VALUE] m\n",
    "- Valid readings: [VALUE]%\n",
    "\n",
    "**Obstacle Detection:**\n",
    "- Threshold: 1.0m\n",
    "- Obstacles detected: [VALUE]\n",
    "\n",
    "### 3.2 Camera Findings\n",
    "\n",
    "**Specifications:**\n",
    "- Resolution: [WIDTH] Ã— [HEIGHT]\n",
    "- FOV: [VALUE] rad\n",
    "- Estimated FPS: [VALUE]\n",
    "\n",
    "**Color Detection:**\n",
    "- Method: HSV thresholding\n",
    "- Overall accuracy: [VALUE]%\n",
    "- Green accuracy: [VALUE]%\n",
    "- Blue accuracy: [VALUE]%\n",
    "- Red accuracy: [VALUE]%\n",
    "\n",
    "### 3.3 Recommendations for Phase 2\n",
    "\n",
    "**LIDAR Processing:**\n",
    "- Use polar coordinates for neural network input\n",
    "- Consider PointNet architecture for 3D point cloud processing\n",
    "- Implement clustering (DBSCAN) for obstacle grouping\n",
    "\n",
    "**Camera Processing:**\n",
    "- HSV thresholding baseline: [ACCURACY]%\n",
    "- Consider CNN for improved robustness (lighting variations)\n",
    "- Augment training data with rotations, brightness adjustments\n",
    "\n",
    "**Sensor Fusion:**\n",
    "- LIDAR provides distance, camera provides color\n",
    "- Combine for robust cube detection and localization\n",
    "- Fuzzy logic can integrate sensor confidences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YouBot (Python 3.14)",
   "language": "python",
   "name": "projeto-final-ia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
